{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http import models\n",
    "from qdrant_client import QdrantClient\n",
    "import time\n",
    "import re\n",
    "from semantic_text_splitter import TiktokenTextSplitter\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import requests\n",
    "import datetime\n",
    "import logging\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#islam :\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://5c32ac64-b1f7-4665-91eb-e321a98c02f6.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
    "    api_key=\"Wd_RTregmznFMCyDLagJHM_7a5TjJJuFLVTuMgfjQD44-BHLnhYbUg\",  # replace with your actual API key\n",
    ")\n",
    "collection_name = \"News_source\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rachid\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://3e4bc828-9c33-4845-bb01-1de4acb3675d.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=\"4ah-1sGbl4OUVUIVSeCtQvHaPIASSjqupe2KoG4G3zxYzkVNp-UJlA\",\n",
    ")\n",
    "collection_name = \"News_source\"\n",
    "guardian_api_key = \"d69e575e-91d0-45c1-97f4-ac0fe69f9899\"\n",
    "duplicates_seuil=100\n",
    "max_consecutive_same_articles=4\n",
    "from_date = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rachi\\AppData\\Local\\Temp\\ipykernel_8468\\193796489.py:1: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant_client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_client.recreate_collection(\n",
    "    collection_name= collection_name,\n",
    "    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE, on_disk=True),\n",
    "    quantization_config=models.ScalarQuantization(\n",
    "        scalar=models.ScalarQuantizationConfig(\n",
    "            type=models.ScalarType.INT8,\n",
    "            always_ram=False,\n",
    "        ),\n",
    "    ),\n",
    "    hnsw_config=models.HnswConfig(on_disk=True),\n",
    "    on_disk_payload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rachi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def remove_tags(input_text):\n",
    "        clean_text = re.sub('<.*?>', '', input_text)\n",
    "        timing_pattern = r'\\b\\d{1,2}\\.\\d{2}[apmAPM]+\\s+[GMTgmt]+\\b' \n",
    "        clean_text = re.sub(timing_pattern, '', clean_text) \n",
    "        clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "        clean_text = clean_text.replace('\\n', ' ')\n",
    "        pattern = r'Â© \\d+ BBC\\. The BBC is not responsible for the content of external sites\\. Read about our approach to external linking\\.'\n",
    "        clean_text = re.sub(pattern, '', clean_text)\n",
    "        return clean_text\n",
    "\n",
    "\n",
    "def process_articles(collected_articles):\n",
    "        chunked_articles = []\n",
    "        chunked_titles = []\n",
    "        chunked_pubdate=[]\n",
    "        for article in collected_articles:  \n",
    "            max_tokens = 512 \n",
    "            splitter = TiktokenTextSplitter(\"gpt-3.5-turbo\", trim_chunks=False)\n",
    "            chunks = splitter.chunks(article['content'], max_tokens)\n",
    "            for chunk in chunks:\n",
    "                chunked_articles.append(chunk)\n",
    "                chunked_titles.append(article['title'])\n",
    "                chunked_pubdate.append(article['publishdate'])\n",
    "        data_dict = {\n",
    "            \"title\": chunked_titles,\n",
    "            \"content\": chunked_articles,\n",
    "            \"publishdate\": chunked_pubdate,\n",
    "        }\n",
    "        dataset = Dataset.from_dict(data_dict)\n",
    "        return dataset\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def _mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "def embed_text(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"content\"], padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**inputs)\n",
    "    pooled_embeds = _mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "    return {\"embedding\": pooled_embeds.cpu().numpy()}\n",
    "\n",
    "def generate_embeddings(dataset):\n",
    "    return dataset.map(embed_text, batched=True, batch_size=128)\n",
    "\n",
    "\n",
    "def _upload_documents_to_Qdrant(data, source):\n",
    "        points = []\n",
    "        for title, content, publishdate, embedding in zip(data[\"title\"], data[\"content\"], data[\"publishdate\"], data[\"embedding\"]):\n",
    "            new_id = str(uuid.uuid4())  # Generate a new UUID for each document\n",
    "            point = models.PointStruct(\n",
    "                id=new_id,\n",
    "                vector=embedding,\n",
    "                payload={\n",
    "                    \"title\": title,\n",
    "                    \"content\": content,\n",
    "                    \"publishdate\": publishdate,\n",
    "                    \"source\" : source\n",
    "                }\n",
    "            )\n",
    "            points.append(point)\n",
    "\n",
    "        qdrant_client.upsert(\n",
    "            collection_name='News_source',\n",
    "            points=points\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Uploaded {len(data['embedding'])} documents to the Qdrant database\")\n",
    "\n",
    "\n",
    "def upload_to_Qdrant(data, batch_size=35, source=''):\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch = data[i:i + batch_size]\n",
    "            _upload_documents_to_Qdrant(batch , source)\n",
    "            logging.info(f\"Uploaded {i + len(batch)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_guardian_news():\n",
    "        ''' \n",
    "        This function fetches news articles from the guardian api\n",
    "        '''\n",
    "        endpoint = 'https://content.guardianapis.com/search'\n",
    "        params = {\n",
    "            'api-key': guardian_api_key,\n",
    "            'show-fields': 'headline,body',\n",
    "            f'from-date': from_date,\n",
    "            'to-date': datetime.datetime.now().strftime('%Y-%m-%d'),\n",
    "            'order-by': 'newest',\n",
    "            'page-size': 200\n",
    "        }\n",
    "        publish_dates=set() # to avoid duplicate articles\n",
    "        count_duplicates_per_request=0\n",
    "        collected_articles = []\n",
    "        consecutive_same_articles = 0\n",
    "        while True:\n",
    "            if len(collected_articles) % 10 == 0: \n",
    "                print(f\"Number of collected documents so far are : {len(collected_articles)} ....\")\n",
    "            response = requests.get(endpoint, params)\n",
    "            data = response.json() \n",
    "            articles = data['response']['results']\n",
    "            previous_article_count = len(collected_articles)\n",
    "            for article in articles:\n",
    "                title = article['fields']['headline']\n",
    "                content = remove_tags(article['fields']['body'])\n",
    "                publish_date=article['webPublicationDate']\n",
    "                if publish_date not in publish_dates:\n",
    "                        publish_dates.add(publish_date)\n",
    "                        collected_articles.append({'title': title, 'content': content, 'publishdate':publish_date})\n",
    "                else:\n",
    "                    count_duplicates_per_request+=1\n",
    "                    continue\n",
    "            if previous_article_count == len(collected_articles):\n",
    "                consecutive_same_articles += 1\n",
    "\n",
    "            if consecutive_same_articles >= max_consecutive_same_articles:  # Replace with desired threshold\n",
    "                break\n",
    "            if count_duplicates_per_request>duplicates_seuil:\n",
    "                print(f\"Number of duplicate articles retrieved from the guardien news source is : {count_duplicates_per_request} , breaking ...\")\n",
    "                break\n",
    "            time.sleep(1)\n",
    "        return process_articles(collected_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of collected documents so far are : 0 ....\n",
      "Number of duplicate articles retrieved from the guardien news source is : 224 , breaking ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd1e922898a456aa9527d8a1c33a6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articles = fetch_guardian_news()\n",
    "embeddings = generate_embeddings(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_Qdrant(data=embeddings, source='guardian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionInfo(status=<CollectionStatus.GREEN: 'green'>, optimizer_status=<OptimizersStatusOneOf.OK: 'ok'>, vectors_count=None, indexed_vectors_count=0, points_count=628, segments_count=2, config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=768, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=True, datatype=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))), payload_schema={})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_client.get_collection(collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_articles_by_source(client, collection_name, source_name, retries=5):\n",
    "    count_result = client.count(\n",
    "        collection_name=collection_name,\n",
    "        count_filter=models.Filter(\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"source\",\n",
    "                    match=models.MatchValue(value=source_name),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    return count_result.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "628"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_articles_by_source(qdrant_client, collection_name, \"guardian\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
